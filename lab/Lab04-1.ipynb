{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모두를 위한 딥러닝 Lab#04-1\n",
    "#### 2018.06.09\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 29293.12 \t [ 1.7626817  0.357805   0.9202516  3.918039  -2.3487532]\n",
      "500 \t 8.055074 \t [153.1626  183.16167 180.61041 199.60104 137.28787]\n",
      "1000 \t 6.8346205 \t [152.78159 183.42703 180.49902 199.47873 137.67133]\n",
      "1500 \t 5.888072 \t [152.4511  183.65779 180.40306 199.36812 138.00893]\n",
      "2000 \t 5.1508226 \t [152.16461 183.85829 180.32054 199.26773 138.3064 ]\n",
      "2500 \t 4.57349 \t [151.91658 184.03241 180.24971 199.17628 138.5688 ]\n",
      "3000 \t 4.118517 \t [151.70209 184.18346 180.18909 199.09271 138.80055]\n",
      "3500 \t 3.7572315 \t [151.51685 184.31442 180.13739 199.0161  139.00551]\n",
      "4000 \t 3.4677022 \t [151.35715 184.42781 180.09343 198.94556 139.18703]\n",
      "4500 \t 3.2332482 \t [151.21971 184.5259  180.05626 198.88039 139.34805]\n",
      "5000 \t 3.041112 \t [151.10173 184.61063 180.02498 198.81996 139.49117]\n",
      "5500 \t 2.8815765 \t [151.00066 184.6837  179.99883 198.76369 139.61858]\n",
      "6000 \t 2.7472196 \t [150.91437 184.7466  179.97714 198.7111  139.73225]\n",
      "6500 \t 2.6323628 \t [150.84091 184.8006  179.95932 198.66176 139.83391]\n",
      "7000 \t 2.532641 \t [150.77872 184.8469  179.94489 198.6153  139.92505]\n",
      "7500 \t 2.444813 \t [150.72629 184.88643 179.93341 198.57143 140.00697]\n",
      "8000 \t 2.3663204 \t [150.68236 184.9201  179.92447 198.52983 140.08081]\n",
      "8500 \t 2.295198 \t [150.64584 184.94864 179.91776 198.49025 140.14757]\n",
      "9000 \t 2.2299583 \t [150.6158  184.9727  179.91295 198.45247 140.20811]\n",
      "9500 \t 2.1694994 \t [150.59135 184.99289 179.90982 198.41632 140.26321]\n",
      "10000 \t 2.1129324 \t [150.57178 185.00967 179.90814 198.3816  140.3135 ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1_data = [73., 93., 89., 96., 73.]\n",
    "x2_data = [80., 88., 91., 98., 66.]\n",
    "x3_data = [75., 93., 90., 100., 70.]\n",
    "y_data = [152., 185., 180., 196., 142.]\n",
    "\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]) ,name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]) ,name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]) ,name='weight3')\n",
    "b = tf.Variable(tf.random_normal([1]) ,name='bias')\n",
    "\n",
    "hypothesis = x1*w1 + x2*w2 + x3*w3 + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(10001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        print(step,\"\\t\",cost_val,\"\\t\",hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 76765.984 \t [[ -89.52955 ]\n",
      " [-113.68388 ]\n",
      " [-108.28532 ]\n",
      " [-123.766174]\n",
      " [ -83.66013 ]]\n",
      "500 \t 7.81529 \t [[155.90678]\n",
      " [182.04527]\n",
      " [182.71617]\n",
      " [193.22414]\n",
      " [142.00845]]\n",
      "1000 \t 6.9687104 \t [[155.5626 ]\n",
      " [182.27612]\n",
      " [182.60423]\n",
      " [193.19333]\n",
      " [142.26917]]\n",
      "1500 \t 6.300109 \t [[155.25943]\n",
      " [182.479  ]\n",
      " [182.50493]\n",
      " [193.17105]\n",
      " [142.49358]]\n",
      "2000 \t 5.7678776 \t [[154.99207]\n",
      " [182.65735]\n",
      " [182.41669]\n",
      " [193.15616]\n",
      " [142.68637]]\n",
      "2500 \t 5.340033 \t [[154.75597]\n",
      " [182.81427]\n",
      " [182.33804]\n",
      " [193.14766]\n",
      " [142.85153]]\n",
      "3000 \t 4.99244 \t [[154.54729]\n",
      " [182.9525 ]\n",
      " [182.26793]\n",
      " [193.14476]\n",
      " [142.99266]]\n",
      "3500 \t 4.7064767 \t [[154.36252]\n",
      " [183.0744 ]\n",
      " [182.20523]\n",
      " [193.14664]\n",
      " [143.11276]]\n",
      "4000 \t 4.4680715 \t [[154.1987 ]\n",
      " [183.182  ]\n",
      " [182.14897]\n",
      " [193.15263]\n",
      " [143.21466]]\n",
      "4500 \t 4.2664237 \t [[154.05316]\n",
      " [183.2771 ]\n",
      " [182.09843]\n",
      " [193.16217]\n",
      " [143.3006 ]]\n",
      "5000 \t 4.093229 \t [[153.92365]\n",
      " [183.36124]\n",
      " [182.05281]\n",
      " [193.17474]\n",
      " [143.37262]]\n",
      "5500 \t 3.942285 \t [[153.80818]\n",
      " [183.43588]\n",
      " [182.01164]\n",
      " [193.18997]\n",
      " [143.43265]]\n",
      "6000 \t 3.8086762 \t [[153.70497]\n",
      " [183.50215]\n",
      " [181.97426]\n",
      " [193.2074 ]\n",
      " [143.48209]]\n",
      "6500 \t 3.6887844 \t [[153.61252]\n",
      " [183.56108]\n",
      " [181.94026]\n",
      " [193.22676]\n",
      " [143.52242]]\n",
      "7000 \t 3.5797622 \t [[153.52946]\n",
      " [183.61368]\n",
      " [181.90918]\n",
      " [193.24768]\n",
      " [143.55481]]\n",
      "7500 \t 3.4794261 \t [[153.45464]\n",
      " [183.66061]\n",
      " [181.88068]\n",
      " [193.26997]\n",
      " [143.58026]]\n",
      "8000 \t 3.3861973 \t [[153.38707]\n",
      " [183.70265]\n",
      " [181.8545 ]\n",
      " [193.29341]\n",
      " [143.59972]]\n",
      "8500 \t 3.2987657 \t [[153.32582]\n",
      " [183.74039]\n",
      " [181.8303 ]\n",
      " [193.31776]\n",
      " [143.614  ]]\n",
      "9000 \t 3.21613 \t [[153.27014]\n",
      " [183.7744 ]\n",
      " [181.8079 ]\n",
      " [193.3429 ]\n",
      " [143.62375]]\n",
      "9500 \t 3.137484 \t [[153.21935]\n",
      " [183.80507]\n",
      " [181.78706]\n",
      " [193.36873]\n",
      " [143.6296 ]]\n",
      "10000 \t 3.062367 \t [[153.17284]\n",
      " [183.83289]\n",
      " [181.7676 ]\n",
      " [193.39497]\n",
      " [143.63205]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix 사용\n",
    "\n",
    "x_data = [[73.,80.,75.],\n",
    "         [93.,88.,93.],\n",
    "         [89.,91.,90.],\n",
    "         [96.,98.,100.],\n",
    "         [73.,66.,70.]]\n",
    "\n",
    "y_data = [[152.],[185.],[180.],[196.],[142.]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(10001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    \n",
    "    if step % 500 == 0 :\n",
    "        print(step,'\\t',cost_val,'\\t',hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
