{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모두를 위한 딥러닝 Lec#03\n",
    "#### 2018.06.08\n",
    "\n",
    "---\n",
    "\n",
    "- Hypothesis and Cost\n",
    "\n",
    "    > Cost를 최소화 하는 것이 목표!\n",
    "    \n",
    "- Simplified hypothesis\n",
    "\n",
    "$ H(x) = Wx $\n",
    "\n",
    "$ cost(W) = \\frac{1}{m}\\sum_{i=1}^m(Wx^i-y^i)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What cost(W) looks like?\n",
    "\n",
    "    - data set\n",
    "        - x = 1, y = 1\n",
    "        - x = 2, y = 2\n",
    "        - x = 3, y = 3\n",
    "        \n",
    "    - W=1, cost(W)=?\n",
    "    > (1/3)((1*1-1)^2 + (1*2-2)^2 + (1*3-3)^2) = 0\n",
    "\n",
    "    - W=0, cost(W)=?\n",
    "    > (1/3)((0*1-1)^2 + (0*2-2)^2 + (0*3-3)^2) = 4.67\n",
    "    \n",
    "    - W=2, cost(W)=?\n",
    "    > (1/3)((2*1-1)^2 + (2*2-2)^2 + (2*3-3)^2) = 1/3 * (1+4+9) = 14/3 = 4.67\n",
    "    \n",
    "    - Cost가 최소인 부분을 찾자!\n",
    "    \n",
    "- Gradient descent algorithm\n",
    "\n",
    "    > 경사 하강 알고리즘\n",
    "\n",
    "    - cost function을 최소화!\n",
    "    - 많은 최소화 문제에 사용된다.\n",
    "    - cost function이 주어졌을 때, W와 b를 찾는 알고리즘\n",
    "    - 어떻게 작동?    \n",
    "        \n",
    "        > 한 점에서 기울기를 구하고, 조금씩 기울기가 낮은 쪽으로 움직이며 최소 값을 구함.\n",
    "\n",
    "        1. 아무 점에서 시작\n",
    "        1. W와 b를 조금씩 바꿔가며 재시도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formal definition\n",
    "\n",
    "$ cost(W) = \\frac{1}{m}\\sum_{i=1}^m(Wx^i-y^i)^2 $ 식에서 미분을 쉽게 하기 위하여 ```2m```으로 나눔\n",
    "\n",
    "$ cost(W) = \\frac{1}{2m}\\sum_{i=1}^m(Wx^i-y^i)^2 $  \n",
    "\n",
    "$ W := W - \\alpha\\frac{\\delta}{\\delta W}cost(W) $  식에서 alpha는 Learning Rate, 상수 약 0.1로 가정.\n",
    "\n",
    "cost함수를 미분.. $ W := W - \\alpha\\frac{\\delta}{\\delta W} \\frac{1}{2m}\\sum_{i=1}^m(Wx^i-y^i)^2 $ \n",
    "\n",
    "$ W := W - \\alpha\\frac{1}{2m}\\sum_{i=1}^m2(Wx^i-y^i)x^i $ 식을 정리하면,\n",
    "\n",
    "\n",
    "#### Gradient Descent Algirithm \n",
    "$ W := W - \\alpha\\frac{1}{m}\\sum_{i=1}^m(Wx^i-y^i)x^i $\n",
    "\n",
    "- Convex Function\n",
    "    - Cost function이 부분최소값을 여러 군데 가질 수 있다. 이러한 경우 경사하강 알고리즘이 올바른 정답을 찾지 못 할 수 있다.\n",
    "    - Cost function을 설계할 때 Convex function 형태가 되는 지 확인을 해야 한다!\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
